From andrewcoppin at btinternet.com  Fri Aug 26 21:30:02 2011
From: andrewcoppin at btinternet.com (Andrew Coppin)
Date: Fri, 26 Aug 2011 20:30:02 +0100
Subject: [Haskell-cafe] bitSize
In-Reply-To: <nnpf57dg3stdf9216d2papsgru76kv3vo4@4ax.com>
References: <4E568D11.30800@btinternet.com> <4E56CD99.6050703@vex.net>
	<CANaM3xBnbOBok9Tc_dxS4hPRMGz_6+nd2dHWgNAc_NmyquNY4w@mail.gmail.com>
	<4E57D6D5.8030407@btinternet.com>
	<nnpf57dg3stdf9216d2papsgru76kv3vo4@4ax.com>
Message-ID: <4E57F43A.9030800@btinternet.com>

On 26/08/2011 07:36 PM, Steve Schafer wrote:
> On Fri, 26 Aug 2011 18:24:37 +0100, you wrote:
>
>> I would usually want #3 or #4.
>
> Out of curiosity, what for? While I do occasionally need to get a
> "logarithmic size estimate" of a number (which is basically what #3 and
> #4 are), the specific requirements in each case tend to vary, enough so
> that it's unlikely that a single function (other than simply taking the
> logarithm) can handle the majority of applications.

You wouldn't want to know how many bits you need to store on disk to 
reliably recreate the value? Or how many bits of randomness you need to 
compute a value less than or equal to this one?

I suppose I could use a binary logarithm. I'm just concerned that it 
would be rather slow. After all, I'm not interested in the exact 
logarithm (which is fractional), just the number of bits (which is a 
small integer)...


